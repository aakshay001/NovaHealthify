{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b229649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Function to extract Product BarCode\n",
    "def get_barcode(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        barcode = soup.find(\"span\", attrs={\"id\":\"barcode\"}) \n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        barcode_value = barcode.text\n",
    "\n",
    "        # Title as a string value\n",
    "        barcode_string = barcode_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        barcode_string = \"\"\n",
    "\n",
    "    return barcode_string\n",
    "\n",
    "# Function to extract Product Brand\n",
    "def get_brand(soup):\n",
    "\n",
    "    try:\n",
    "        brand = soup.find(\"span\", attrs={\"id\":\"field_brands_value\"}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            brand = soup.find(\"span\", attrs={\"id\":\"field_brands_value\"}).text.strip()\n",
    "\n",
    "        except:\n",
    "            brand = \"\"\n",
    "\n",
    "    return brand\n",
    "\n",
    "# Function to extract Product name\n",
    "def get_product(soup):\n",
    "\n",
    "    try:\n",
    "        p = soup.find(\"h1\", attrs={\"class\":\"title-3\"})\n",
    "        product = p.text.strip()\n",
    "    \n",
    "#     except AttributeError:\n",
    "#         try:\n",
    "#             product = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "    except:\n",
    "            product = \"\"\n",
    "\n",
    "    return product\n",
    "\n",
    "# Function to extract Category\n",
    "def get_category(soup):\n",
    "    try:\n",
    "        category = soup.find(\"span\", attrs={\"id\":\"field_categories_value\"}).text.strip().split(\",\")[:2]\n",
    "        category = \", \".join(category)\n",
    "    except AttributeError:\n",
    "        category = \"\"\t\n",
    "\n",
    "    return category\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_subcategory(soup):\n",
    "    try:\n",
    "        subcategory = soup.find(\"span\", attrs={\"id\":\"field_categories_value\"}).text.strip().split(\",\")[-2:]\n",
    "        subcategory = \", \".join(subcategory)\n",
    "    except AttributeError:\n",
    "        subcategory = \"\"\t\n",
    "\n",
    "    return subcategory\n",
    "\n",
    "def get_ingredients(soup):\n",
    "    try:\n",
    "        ingredients = soup.find(\"div\", attrs={\"class\":\"panel_text\"}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        ingredients = \"\"\n",
    "\n",
    "    return ingredients\n",
    "\n",
    "def get_allergens(soup):\n",
    "    try:\n",
    "        allergens = soup.find(\"span\", attrs={\"class\":\"allergen\"}).text.strip()\n",
    "#         allergens = available.find(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        allergens = \"\"\n",
    "\n",
    "    return allergens\n",
    "\n",
    "def get_nova3(soup):\n",
    "    try:\n",
    "        s = soup.find(\"h4\", attrs={\"class\":None}).text.strip()\n",
    "        if s == \"Processed foods\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova3 = \", \".join(nova)\n",
    "            return nova3\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "        \n",
    "def get_nova4(soup):\n",
    "    try:\n",
    "        s = soup.find(\"h4\", attrs={\"class\":None}).text.strip()\n",
    "        if s == \"Ultra processed foods\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova4 = \", \".join(nova)\n",
    "            return nova4\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "        \n",
    "def get_nova(soup):\n",
    "    try:\n",
    "        s = soup.find(\"h4\", attrs={\"class\":None}).text.strip()\n",
    "        if s == \"Ultra processed foods\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova4 = \", \".join(nova)\n",
    "            return s\n",
    "\n",
    "        elif s == \"Processed foods\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova3 = \", \".join(nova)\n",
    "            return s\n",
    "        \n",
    "        elif s == \"Processed culinary ingredients\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova2 = \", \".join(nova)\n",
    "            return s\n",
    "        \n",
    "        elif s == \"Unprocessed or minimally processed foods\" :\n",
    "            elements = soup.find(\"div\", attrs={\"class\":\"content panel_content\"}).find_all('li')\n",
    "            nova = []\n",
    "            for element in elements: \n",
    "                if \":\" in element.text  :\n",
    "                    nova.append(element.text)\n",
    "            nova1 = \", \".join(nova)\n",
    "            return s\n",
    "        else:\n",
    "            return s\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "\n",
    "def time_difference(start,end):\n",
    "    time_diff = end-start\n",
    "    days = time_diff.days\n",
    "    seconds = time_diff.seconds\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"Time taken to Complete:{hours} hours, {minutes} minutes, {seconds} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Page to start from: 0\n",
      "Enter Page to end: 69\n",
      "Page 0 links exctracted.\n",
      "Page 1 links exctracted.\n",
      "Page 2 links exctracted.\n",
      "Page 3 links exctracted.\n",
      "Page 4 links exctracted.\n",
      "Page 5 links exctracted.\n",
      "Page 6 links exctracted.\n",
      "Page 7 links exctracted.\n",
      "Page 8 links exctracted.\n",
      "Page 9 links exctracted.\n",
      "Page 10 links exctracted.\n",
      "Page 11 links exctracted.\n",
      "Page 12 links exctracted.\n",
      "Page 13 links exctracted.\n",
      "Page 14 links exctracted.\n",
      "Page 15 links exctracted.\n",
      "Page 16 links exctracted.\n",
      "Page 17 links exctracted.\n",
      "Page 18 links exctracted.\n",
      "Page 19 links exctracted.\n",
      "Page 20 links exctracted.\n",
      "Page 21 links exctracted.\n",
      "Page 22 links exctracted.\n",
      "Page 23 links exctracted.\n",
      "Page 24 links exctracted.\n",
      "Page 25 links exctracted.\n",
      "Page 26 links exctracted.\n",
      "Page 27 links exctracted.\n",
      "Page 28 links exctracted.\n",
      "Page 29 links exctracted.\n",
      "Page 30 links exctracted.\n",
      "Page 31 links exctracted.\n",
      "Page 32 links exctracted.\n",
      "Page 33 links exctracted.\n",
      "Page 34 links exctracted.\n",
      "Page 35 links exctracted.\n",
      "Page 36 links exctracted.\n",
      "Page 37 links exctracted.\n",
      "Page 38 links exctracted.\n",
      "Page 39 links exctracted.\n",
      "Page 40 links exctracted.\n",
      "Page 41 links exctracted.\n",
      "Page 42 links exctracted.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "start_page = int(input(\"Enter Page to start from: \"))\n",
    "end_page = int(input(\"Enter Page to end: \"))\n",
    "all_products = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    URL = f'https://world.openfoodfacts.org/{page}'\n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "    webpage = requests.get(URL, headers = HEADERS)\n",
    "    \n",
    "    # webpage.content\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", attrs={\"class\": None})\n",
    "    \n",
    "    href_lst = []\n",
    "    for i in links:\n",
    "        href_lst.append(i.get('href'))\n",
    "    for p in href_lst:\n",
    "        if (p is not None) and (re.match(\"/product/(\\d+)/([\\w-]+)\",p)is not None):\n",
    "            all_products.append(p)\n",
    "    print(f\"Page {page} links exctracted.\")\n",
    "print(\"All Links Generated.\")\n",
    "df = { \"BRAND NAME\":[], \"PRODUCT NAME\":[], \"CATEGORY\":[],\"SUBCATEGORY\":[],\"INGREDIENTS\":[],\"NOVA Group\":[],}\n",
    "#       \"NOVA 3\":[], \"NOVA 4\":[]}\n",
    "print(f\"Products to be added: {len(all_products)}\")\n",
    "c= 0\n",
    "# Loop for extracting product details from each link \n",
    "for link in all_products:\n",
    "    new_webpage = requests.get(\"https://in.openfoodfacts.org\" + link, headers=HEADERS)\n",
    "\n",
    "    new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "    # Function calls to display all necessary product information\n",
    "#     df['BARCODE'].append(get_barcode(new_soup))\n",
    "    df['BRAND NAME'].append(get_brand(new_soup))\n",
    "    df['PRODUCT NAME'].append(get_product(new_soup))\n",
    "    df['CATEGORY'].append(get_category(new_soup))\n",
    "    df['SUBCATEGORY'].append(get_subcategory(new_soup))\n",
    "    df['INGREDIENTS'].append(get_ingredients(new_soup))\n",
    "    df['NOVA Group'].append(get_nova(new_soup))\n",
    "#     df['NOVA 3'].append(get_nova3(new_soup))\n",
    "#     df['NOVA 4'].append(get_nova4(new_soup))\n",
    "    c += 1\n",
    "    print(f\"Product {c} Added.\")\n",
    "print(\"All items Added.\")\n",
    "\n",
    "nova_df = pd.DataFrame.from_dict(df)\n",
    "nova_df['BRAND NAME'].replace('', np.nan, inplace=True)\n",
    "nova_df = nova_df.dropna(subset=['BRAND NAME'])\n",
    "\n",
    "# writer = pd.ExcelWriter(\"OpenFoodFacts.xlsx\", engine='xlsxwriter')\n",
    "# nova_df.to_excel(\"OpenFoodFactsNOVA.xlsx\",sheet_name = \"SOLID FOOD\", header=True, index=False)\n",
    "# end = datetime.now()\n",
    "# print(time_difference(start,end))\n",
    "# print(\"Excel Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = pd.ExcelWriter(\"OpenFoodFacts.xlsx\", engine='xlsxwriter')\n",
    "file_name = f\"OpenFoodFactsNOVA_{start_page}-{end_page}.csv\" \n",
    "nova_df.to_csv(file_name,sheet_name = \"SOLID FOOD\", header=True, index=False)\n",
    "end = datetime.now()\n",
    "print(time_difference(start,end))\n",
    "print(\"Excel Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e04d3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nova_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nova_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nova_df' is not defined"
     ]
    }
   ],
   "source": [
    "nova_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95afb4c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_difference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time_difference(start,end)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_difference' is not defined"
     ]
    }
   ],
   "source": [
    "time_difference(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df06d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://openfoodfacts-images.s3.eu-west-3.amazonaws.com/data/401/235/911/4303/1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14634adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
